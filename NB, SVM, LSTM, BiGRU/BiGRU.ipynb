{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"BiGRU.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"fJQuEYnlO_Ol","executionInfo":{"status":"ok","timestamp":1601373990373,"user_tz":-420,"elapsed":44583,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"c1a64994-af8c-4ba7-e34b-362997fc6895","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import platform\n","import datetime,pytz\n","\n","root_ = '/content/drive/My Drive/colab/' if platform.system() == 'Linux' else '/Users/love/Test/'\n","\n","chinese_text_classifier_ = os.path.join(root_, 'ChineseTextClassifier')\n","\n","if not os.path.exists(chinese_text_classifier_):\n","    os.makedirs(chinese_text_classifier_)\n","\n","data_ = os.path.join(chinese_text_classifier_, 'data')\n","if not os.path.exists(data_):\n","    os.makedirs(data_)\n","\n","wordJson_ = os.path.join(data_, 'wordJson')\n","if not os.path.exists(wordJson_):\n","    os.makedirs(wordJson_)\n","   \n","bigru_model_ = os.path.join(chinese_text_classifier_, 'bigru_model')\n","if not os.path.exists(bigru_model_):\n","    os.makedirs(bigru_model_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mky3JpHoO5Bp"},"source":["# GRU+attention实现中文商品评论二分类"]},{"cell_type":"code","metadata":{"id":"ZmBev_9wO5Bq","executionInfo":{"status":"ok","timestamp":1601373992461,"user_tz":-420,"elapsed":46650,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"c68cf771-145d-4499-d0bf-967776ca51f1","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","print(tf.keras.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n","2.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1NScxqeeO5Bu"},"source":["import os\n","import csv\n","import time\n","import datetime\n","import random\n","import json\n","from collections import Counter\n","from math import sqrt\n","import gensim\n","import pandas as pd\n","import numpy as np\n","\n","\n","from tensorflow.keras import backend\n","from tensorflow.keras.layers import Layer,TimeDistributed,Input,Conv2D,MaxPool2D,concatenate,Flatten,Dense,Dropout,Embedding,Reshape,GRU\n","from tensorflow.keras import Sequential,optimizers,losses\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n","from bs4 import BeautifulSoup\n","import logging\n","import gensim\n","from gensim.models import word2vec\n","from gensim.models.word2vec import Word2Vec\n","\n","import multiprocessing\n","import yaml\n","import jieba"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2Fa3XLFO5Bx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m0e3Qvp1O5Bz"},"source":["# 参数配置"]},{"cell_type":"code","metadata":{"id":"f9Z73dH1O5B0"},"source":["class Config(object):\n","    \n","    #数据集路径\n","    dataSource = os.path.join(data_, 'dataset.txt')\n","    stopWordSource = os.path.join(data_, 'stopword.txt')\n","    \n","    \n","    #分词后保留大于等于最低词频的词\n","    miniFreq=1\n","    \n","    \n","    #统一输入文本序列的定长，取了所有序列长度的均值。超出将被截断，不足则补0\n","    sequenceLength = 200  \n","    batchSize=64\n","    epochs=10\n","    \n","    numClasses = 2\n","    #训练集的比例\n","    rate = 0.8  \n","    \n","    \n","    #生成嵌入词向量的维度\n","    embeddingSize = 150\n","    \n","    #卷积核数\n","    numFilters = 30\n","    \n","    #卷积核大小\n","    filterSizes = [2,3,4,5]\n","    dropoutKeepProb = 0.5\n","    \n","    #L2正则系数\n","    l2RegLambda = 0.1\n","    \n","   \n","\n","    \n","# 实例化配置参数对象\n","config = Config()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYpzBx12O5B2","executionInfo":{"status":"ok","timestamp":1601373993670,"user_tz":-420,"elapsed":47837,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"baa1398a-aa1b-4e8c-eeb0-cac11cc7b562","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["config.batchSize"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"ThxeFfeiO5B5"},"source":["# 预训练词向量"]},{"cell_type":"code","metadata":{"id":"EsxAJRYwO5B5","executionInfo":{"status":"ok","timestamp":1601375300054,"user_tz":-420,"elapsed":1354210,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"9e88d046-34da-4155-9440-5d4640724e81","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["#中文语料\n","#设置输出日志\n","#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","file = open(os.path.join(data_, 'dataset.txt')) \n","sentences=[]\n","for line in file:\n","    temp=line.replace('\\n','').split(',@=,')\n","    sentences.append(jieba.lcut(temp[0]))\n","file.close()\n","\n","\n","model = word2vec.Word2Vec(sentences,size=config.embeddingSize,\n","                     min_count=config.miniFreq,\n","                     window=10,\n","                     workers=multiprocessing.cpu_count(),sg=1,\n","                     iter=20)\n","model.save(os.path.join(data_, 'word2VecModel'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.979 seconds.\n","Prefix dict has been built successfully.\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"agP7ercaO5B8","executionInfo":{"status":"ok","timestamp":1601375301635,"user_tz":-420,"elapsed":1355779,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"939da45a-557c-4187-8c79-0d72ac8e5448","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["model = gensim.models.Word2Vec.load(os.path.join(data_, 'word2VecModel'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"n6Q-saGDO5CB"},"source":["# 数据预处理"]},{"cell_type":"code","metadata":{"id":"JwAyNCMrO5CB","executionInfo":{"status":"ok","timestamp":1601375607731,"user_tz":-420,"elapsed":48113,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"e738619b-726e-49e9-bd2b-7fc4b381f6e1","colab":{"base_uri":"https://localhost:8080/","height":564}},"source":["# 数据预处理的类，生成训练集和测试集\n","class Dataset(object):\n","    def __init__(self, config):\n","        self.dataSource = config.dataSource\n","        self.stopWordSource = config.stopWordSource  \n","        \n","        # 每条输入的序列处理为定长\n","        self.sequenceLength = config.sequenceLength  \n","        \n","        self.embeddingSize = config.embeddingSize\n","        self.batchSize = config.batchSize\n","        self.rate = config.rate\n","        self.miniFreq=config.miniFreq\n","        \n","        self.stopWordDict = {}\n","        \n","        self.trainReviews = []\n","        self.trainLabels = []\n","        \n","        self.evalReviews = []\n","        self.evalLabels = []\n","        \n","        self.wordEmbedding =None\n","        self.n_symbols=0\n","        \n","        self.wordToIndex = {}\n","        self.indexToWord = {}\n","        \n","        \n","        \n","    def readData(self, filePath):\n","        file = open(filePath) \n","        text=[]\n","        label=[]\n","        for line in file:\n","            temp=line.replace('\\n','').split(',@=,')\n","            text.append(temp[0])\n","            label.append(temp[1])\n","        file.close()\n","        \n","        print('data:',len(text),len(label))\n","        texts = [jieba.lcut(document.replace('\\n', '')) for document in text]\n","\n","        return texts, label\n","\n","    \n","    def readStopWord(self, stopWordPath):\n","        \"\"\"\n","        读取停用词\n","        \"\"\"\n","        \n","        with open(stopWordPath, \"r\") as f:\n","            stopWords = f.read()\n","            stopWordList = stopWords.splitlines()\n","            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n","            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n","    \n","    \n","    def getWordEmbedding(self, words):\n","        \"\"\"\n","        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n","        \"\"\"\n","        \n","        #中文\n","        model = gensim.models.Word2Vec.load(os.path.join(data_, 'word2VecModel'))\n","        \n","        vocab = []\n","        wordEmbedding = []\n","        \n","        # 添加 \"pad\" 和 \"UNK\", \n","        vocab.append(\"pad\")\n","        wordEmbedding.append(np.zeros(self.embeddingSize))\n","        \n","        vocab.append(\"UNK\")\n","        wordEmbedding.append(np.random.randn(self.embeddingSize))\n","        \n","        for word in words:\n","            try:\n","                \n","                #中文\n","                vector =model[word]\n","                \n","                vocab.append(word)\n","                wordEmbedding.append(vector)\n","            except:\n","                \n","                print(word + \" : 不存在于词向量中\")\n","                \n","        return vocab, np.array(wordEmbedding)\n","    \n","    \n","    \n","    def genVocabulary(self, reviews):\n","        \"\"\"\n","        生成词向量和词汇-索引映射字典，可以用全数据集\n","        \"\"\"\n","        \n","        allWords = [word for review in reviews for word in review]\n","        \n","        # 去掉停用词\n","        subWords = [word for word in allWords if word not in self.stopWordDict]\n","        \n","        wordCount = Counter(subWords)  # 统计词频，排序\n","        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n","        \n","        # 去除低频词\n","        words = [item[0] for item in sortWordCount if item[1] >= self.miniFreq ]\n","        \n","        \n","        #获取词列表和顺序对应的预训练权重矩阵\n","        vocab, wordEmbedding = self.getWordEmbedding(words)\n","        \n","        self.wordEmbedding = wordEmbedding\n","        \n","        self.wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n","        self.indexToWord = dict(zip(list(range(len(vocab))), vocab))\n","        self.n_symbols = len(self.wordToIndex) + 1\n","        \n","        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n","        with open(os.path.join(wordJson_, 'wordToIndex.json'), \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.wordToIndex, f)\n","        \n","        with open(os.path.join(wordJson_, 'indexToWord.json'), \"w\", encoding=\"utf-8\") as f:\n","            json.dump(self.indexToWord, f)\n","\n","\n","    def reviewProcess(self, review, sequenceLength, wordToIndex):\n","        \"\"\"\n","        将数据集中的每条评论里面的词，根据词表，映射为index表示\n","        每条评论 用index组成的定长数组来表示\n","        \n","        \"\"\"\n","        \n","        reviewVec = np.zeros((sequenceLength))\n","        sequenceLen = sequenceLength\n","        \n","        # 判断当前的序列是否小于定义的固定序列长度\n","        if len(review) < sequenceLength:\n","            sequenceLen = len(review)\n","            \n","        for i in range(sequenceLen):\n","            if review[i] in wordToIndex:\n","                reviewVec[i] = wordToIndex[review[i]]\n","            else:\n","                reviewVec[i] = wordToIndex[\"UNK\"]\n","\n","        return reviewVec\n","\n","    \n","    \n","    \n","    def genTrainEvalData(self, x, y, rate):\n","        \"\"\"\n","        生成训练集和验证集\n","        \"\"\"\n","        \n","        reviews = []\n","        labels = []\n","        \n","        # 遍历所有的文本，将文本中的词转换成index表示\n","        for i in range(len(x)):\n","            \n","            reviewVec = self.reviewProcess(x[i], self.sequenceLength, self.wordToIndex)\n","            reviews.append(reviewVec)\n","            \n","            labels.append([y[i]])\n","            \n","        trainIndex = int(len(x) * rate)\n","        \n","       \n","        #trainReviews = sequence.pad_sequences(reviews[:trainIndex], maxlen=self.sequenceLength)\n","        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n","        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n","        trainLabels = to_categorical(trainLabels,num_classes=2) \n","        \n","        #evalReviews = sequence.pad_sequences(reviews[trainIndex:], maxlen=self.sequenceLength)\n","        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n","        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n","        print(evalLabels[:3])\n","        evalLabels = to_categorical(evalLabels,num_classes=2) \n","        print(evalLabels[:3])\n","        return trainReviews, trainLabels, evalReviews, evalLabels\n","        \n","        \n","        \n"," \n","            \n","    def dataGen(self):\n","        \"\"\"\n","        初始化训练集和验证集\n","        \"\"\"\n","        \n","        #读取停用词\n","        self.readStopWord(self.stopWordSource)\n","        \n","        #读取数据集\n","        reviews, labels = self.readData(self.dataSource)\n","        \n","        #分词、去停用词\n","        #生成 词汇-索引 映射表和预训练权重矩阵，并保存\n","        self.genVocabulary(reviews)\n","        \n","        \n","        #初始化训练集和测试集\n","        trainReviews, trainLabels, evalReviews, evalLabels = self.genTrainEvalData(reviews, labels, self.rate)\n","        self.trainReviews = trainReviews\n","        self.trainLabels = trainLabels\n","        \n","        self.evalReviews = evalReviews\n","        self.evalLabels = evalLabels\n","        \n","        \n","data = Dataset(config)\n","data.dataGen()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data: 119988 119988\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["真以 : 不存在于词向量中\n","多咯 : 不存在于词向量中\n","快完了 : 不存在于词向量中\n","Atat : 不存在于词向量中\n","rk : 不存在于词向量中\n","Havaalan : 不存在于词向量中\n","zO1RKFo : 不存在于词向量中\n","冷得直 : 不存在于词向量中\n","inches : 不存在于词向量中\n","土银 : 不存在于词向量中\n","高卢 : 不存在于词向量中\n","laurendancer : 不存在于词向量中\n","YeemanL : 不存在于词向量中\n","彭惠娟 : 不存在于词向量中\n","Christo : 不存在于词向量中\n","he1r76 : 不存在于词向量中\n","aoS1HS : 不存在于词向量中\n","汽车网 : 不存在于词向量中\n","横评 : 不存在于词向量中\n","武文涛 : 不存在于词向量中\n","晚景凄凉 : 不存在于词向量中\n","[[1.]\n"," [0.]\n"," [1.]]\n","[[0. 1.]\n"," [1. 0.]\n"," [0. 1.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9sWABddwauby","executionInfo":{"status":"ok","timestamp":1601375553263,"user_tz":-420,"elapsed":940,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"28bc8835-39be-412c-c4ec-59ad17decdf8","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(\"train data shape: {}\".format(data.trainReviews.shape))\n","print(\"train label shape: {}\".format(data.trainLabels.shape))\n","print(\"eval data shape: {}\".format(data.evalReviews.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train data shape: (95990, 200)\n","train label shape: (95990, 2)\n","eval data shape: (23998, 200)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lDers6vOO5CG"},"source":["# 定义网络结构"]},{"cell_type":"code","metadata":{"id":"u3UJZlYXO5CG","executionInfo":{"status":"ok","timestamp":1601375991951,"user_tz":-420,"elapsed":2286,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"4e1bb94b-f9a7-4256-8bb5-a0e0739da42d","colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["class AttentionLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(** kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape)==3\n","        # W.shape = (time_steps, time_steps)\n","        self.W = self.add_weight(name='att_weight', \n","                                 shape=(input_shape[1], input_shape[1]),\n","                                 initializer='uniform',\n","                                 trainable=True)\n","        self.b = self.add_weight(name='att_bias', \n","                                 shape=(input_shape[1],),\n","                                 initializer='uniform',\n","                                 trainable=True)\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        # inputs.shape = (batch_size, time_steps, seq_len)\n","        x = backend.permute_dimensions(inputs, (0, 2, 1))\n","        # x.shape = (batch_size, seq_len, time_steps)\n","        a = backend.softmax(backend.tanh(backend.dot(x, self.W) + self.b))\n","        outputs = backend.permute_dimensions(a * x, (0, 2, 1))\n","        outputs = backend.sum(outputs, axis=1)\n","        return outputs\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[2]\n","\n","\n","\n","def bigru(n_symbols,embedding_weights,config):\n","    \n","    model =Sequential([\n","        Embedding(input_dim=n_symbols, output_dim=config.embeddingSize,\n","                        weights=[embedding_weights],\n","                        input_length=config.sequenceLength),\n","        \n","    #LSTM层\n","    #LSTM(50,activation='tanh', dropout=0.5, recurrent_dropout=0.5,kernel_regularizer=regularizers.l2(config.model.l2RegLambda)),\n","    GRU(50,activation='tanh', dropout=0.5, recurrent_dropout=0.5,return_sequences=True),\n","    Dropout(config.dropoutKeepProb),\n","    AttentionLayer(),\n","    Dense(2, activation='softmax')])\n","    \n","    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","    return model\n","    \n","    \n","    \n","wordEmbedding = data.wordEmbedding\n","n_symbols=data.n_symbols\n","model = bigru(n_symbols,wordEmbedding,config)\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 200, 150)          30130800  \n","_________________________________________________________________\n","gru (GRU)                    (None, 200, 50)           30300     \n","_________________________________________________________________\n","dropout (Dropout)            (None, 200, 50)           0         \n","_________________________________________________________________\n","attention_layer (AttentionLa (None, 50)                40200     \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 102       \n","=================================================================\n","Total params: 30,201,402\n","Trainable params: 30,201,402\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PJ8AvtGBO5CJ"},"source":["# 训练模型"]},{"cell_type":"code","metadata":{"id":"gZJcdu47O5CJ","executionInfo":{"status":"ok","timestamp":1601381113992,"user_tz":-420,"elapsed":5084931,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"811214c3-8390-47c7-f943-bf68f839e7be","colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["x_train = data.trainReviews\n","y_train = data.trainLabels\n","x_eval = data.evalReviews\n","y_eval = data.evalLabels\n","\n","wordEmbedding = data.wordEmbedding\n","n_symbols=data.n_symbols\n","\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n","model_checkpoint = ModelCheckpoint(os.path.join(bigru_model_, 'best_model','model_{epoch:02d}-{val_accuracy:.2f}.hdf5'), save_best_only=True, save_weights_only=True)\n","history = model.fit(x_train, y_train, batch_size=config.batchSize, epochs=config.epochs, validation_split=0.3,shuffle=True, callbacks=[reduce_lr,early_stopping,model_checkpoint])\n","#验证\n","\n","scores = model.evaluate(x_eval, y_eval)\n","\n","#保存模型\n","yaml_string = model.to_yaml()\n","with open(os.path.join(bigru_model_, 'bigru.yml'), 'w') as outfile:\n","    outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n","model.save_weights(os.path.join(bigru_model_, 'bigru.h5'))\n","\n","\n","print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","1050/1050 [==============================] - 722s 688ms/step - loss: 0.1627 - accuracy: 0.9372 - val_loss: 0.0887 - val_accuracy: 0.9800\n","Epoch 2/10\n","1050/1050 [==============================] - 722s 688ms/step - loss: 0.0705 - accuracy: 0.9814 - val_loss: 0.0773 - val_accuracy: 0.9806\n","Epoch 3/10\n","1050/1050 [==============================] - 719s 685ms/step - loss: 0.0659 - accuracy: 0.9801 - val_loss: 0.0807 - val_accuracy: 0.9806\n","Epoch 4/10\n","1050/1050 [==============================] - 719s 685ms/step - loss: 0.0515 - accuracy: 0.9825 - val_loss: 0.0799 - val_accuracy: 0.9774\n","Epoch 5/10\n","1050/1050 [==============================] - 717s 683ms/step - loss: 0.0404 - accuracy: 0.9844 - val_loss: 0.0890 - val_accuracy: 0.9773\n","Epoch 6/10\n","1050/1050 [==============================] - 724s 690ms/step - loss: 0.0324 - accuracy: 0.9872 - val_loss: 0.1002 - val_accuracy: 0.9734\n","Epoch 7/10\n","1050/1050 [==============================] - 723s 688ms/step - loss: 0.0269 - accuracy: 0.9887 - val_loss: 0.1093 - val_accuracy: 0.9721\n","750/750 [==============================] - 29s 39ms/step - loss: 0.1073 - accuracy: 0.9735\n","test_loss: 0.107263, accuracy: 0.973539\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GpUdX2A2xtJy","executionInfo":{"status":"ok","timestamp":1601599754598,"user_tz":-420,"elapsed":1011,"user":{"displayName":"fuhong tang","photoUrl":"","userId":"13608114256629551529"}},"outputId":"c4406340-c681-47a9-f77a-cfadd9105fd6","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["result = model.predict(x_eval)\n","\n","result = np.argmax(result, axis=1)\n","y_eval = np.argmax(y_eval, axis=1)\n","\n","from sklearn import metrics\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","print('acc:',accuracy_score(y_eval, result))\n","print('pc',precision_score(y_eval, result))\n","print('rc:',recall_score(y_eval, result))\n","print('f1:',f1_score(y_eval, result))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["acc: 0.9255794616218018\n","pc 0.9033518121715545\n","rc: 0.951666242813099\n","f1: 0.9268882066473383\n"],"name":"stdout"}]}]}