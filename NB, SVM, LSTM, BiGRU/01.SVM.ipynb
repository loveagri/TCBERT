{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"01.SVM.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kz6jVUc0CuQS"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import platform\n","import datetime,pytz\n","\n","root_ = '/content/drive/My Drive/colab/' if platform.system() == 'Linux' else '/Users/love/Test/'\n","\n","WeiboSentiment_ = os.path.join(root_, 'WeiboSentiment')\n","if not os.path.exists(WeiboSentiment_):\n","    os.makedirs(WeiboSentiment_)\n","\n","model_ = os.path.join(WeiboSentiment_, 'model')\n","if not os.path.exists(model_):\n","    os.makedirs(model_)\n","\n","\n","import jieba\n","import re\n","import numpy as np\n","\n","def tokenize(text):\n","    \"\"\"\n","    带有语料清洗功能的分词函数, 包含数据预处理, 可以根据自己的需求重载\n","    \"\"\"\n","    text = re.sub(\"\\{%.+?%\\}\", \" \", text)           # 去除 {%xxx%} (地理定位, 微博话题等)\n","    text = re.sub(\"@.+?( |$)\", \" \", text)           # 去除 @xxx (用户名)\n","    text = re.sub(\"【.+?】\", \" \", text)              # 去除 【xx】 (里面的内容通常都不是用户自己写的)\n","    icons = re.findall(\"\\[.+?\\]\", text)             # 提取出所有表情图标\n","    text = re.sub(\"\\[.+?\\]\", \"IconMark\", text)      # 将文本中的图标替换为`IconMark`\n","\n","    tokens = []\n","    for k, w in enumerate(jieba.lcut(text)):\n","        w = w.strip()\n","        if \"IconMark\" in w:                         # 将IconMark替换为原图标\n","            for i in range(w.count(\"IconMark\")):\n","                tokens.append(icons.pop(0))\n","        elif w and w != '\\u200b' and w.isalpha():   # 只保留有效文本\n","                tokens.append(w)\n","    return tokens\n","\n","\n","def load_curpus(path):\n","    \"\"\"\n","    加载语料库\n","    \"\"\"\n","    data = []\n","    with open(path, \"r\", encoding=\"utf8\") as f:\n","        for line in f:\n","            [_, seniment, content] = line.split(\",\", 2)\n","            content = tokenize(content)             # 分词\n","            data.append((content, int(seniment)))\n","    return data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2SaPnuMCAmlB"},"source":["#### 加载数据"]},{"cell_type":"code","metadata":{"id":"Ulh-J8A8AmlC"},"source":["import pandas as pd\n","train_data = load_curpus(os.path.join(WeiboSentiment_, 'train.txt'))\n","test_data = load_curpus(os.path.join(WeiboSentiment_, 'test.txt'))\n","train_df = pd.DataFrame(train_data, columns=[\"content\", \"sentiment\"])\n","test_df = pd.DataFrame(test_data, columns=[\"content\", \"sentiment\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kz05jBQ_AmlG"},"source":["加载停用词"]},{"cell_type":"code","metadata":{"id":"3pCoZH_HAmlH"},"source":["stopwords = []\n","with open(os.path.join(WeiboSentiment_, 'stopwords.txt'), \"r\", encoding=\"utf8\") as f:\n","    for w in f:\n","        stopwords.append(w.strip())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FvIUQyhwAmlL"},"source":["TfIdf"]},{"cell_type":"code","metadata":{"id":"7O1eEioxAmlM"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","data_str = [\" \".join(content) for content, sentiment in train_data] + \\\n","            [\" \".join(content) for content, sentiment in test_data]\n","tfidf = TfidfVectorizer(token_pattern='\\[?\\w+\\]?', stop_words=stopwords)\n","tfidf_fit = tfidf.fit_transform(data_str)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QK0pPclAmlQ"},"source":["加载之前训练好的FastText模型"]},{"cell_type":"code","metadata":{"id":"I299CsbjAmlR"},"source":["!pip install gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AS3ObTFoAmlc"},"source":["from gensim.models import FastText\n","model = FastText.load(os.path.join(model_, 'model_100.txt'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukfh_ftvAmlf"},"source":["最多只保留Tf-Idf最高的前多少个词"]},{"cell_type":"code","metadata":{"id":"j15R3g0_Amlg"},"source":["key_words = 30"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6AuUyjqAmlj"},"source":["#### 用每个词的Tfidf作为权重, 对FastText词向量进行加权, 得到表征每个句子的向量"]},{"cell_type":"code","metadata":{"id":"0l6GtZwdAmlj"},"source":["X_train, y_train = [], []\n","for content, sentiment in train_data:\n","    X, y = [], sentiment\n","    X_tfidf = tfidf.transform([\" \".join(content)]).toarray()\n","    keywords_index = np.argsort(-X_tfidf)[0, :key_words]\n","    for w in content:\n","        if w in model and w in tfidf.vocabulary_ and tfidf.vocabulary_[w] in keywords_index:\n","            X.append(np.expand_dims(model[w], 0) * X_tfidf[0, tfidf.vocabulary_[w]])\n","    if X:\n","        X = np.concatenate(X)\n","        X = np.mean(X, axis=0)\n","        X_train.append(X)\n","        y_train.append(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GsboCWKgAmln"},"source":["X_test, y_test = [], []\n","for content, sentiment in test_data:\n","    X, y = [], sentiment\n","    X_tfidf = tfidf.transform([\" \".join(content)]).toarray()\n","    keywords_index = np.argsort(-X_tfidf)[0, :key_words]\n","    for w in content:\n","        if w in model and w in tfidf.vocabulary_ and tfidf.vocabulary_[w] in keywords_index:\n","            X.append(np.expand_dims(model[w], 0) * X_tfidf[0, tfidf.vocabulary_[w]])\n","    if X:\n","        X = np.concatenate(X)\n","        X = np.mean(X, axis=0)\n","        X_test.append(X)\n","        y_test.append(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35-agAjkAmlq"},"source":["### SVM"]},{"cell_type":"code","metadata":{"id":"r-t3Nb0eAmlr"},"source":["from sklearn import svm\n","clf = svm.SVC(C=1, class_weight={1: .95, 0: 1.})\n","clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFeUfwt0Amlu"},"source":["result = clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCRgjszdAmlw"},"source":["from sklearn import metrics\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","print('acc:',accuracy_score(y_test, result))\n","print('pc',precision_score(y_test, result))\n","print('rc:',recall_score(y_test, result))\n","print('f1:',f1_score(y_test, result))"],"execution_count":null,"outputs":[]}]}