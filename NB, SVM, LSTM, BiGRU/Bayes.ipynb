{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"colab":{"name":"02.Bayes.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"pS1_m0GbIaQ2","executionInfo":{"status":"ok","timestamp":1601373440862,"user_tz":-420,"elapsed":16574,"user":{"displayName":"loveagri tang","photoUrl":"","userId":"02820716366022293214"}},"outputId":"ab127888-a2ac-40ed-927c-ab5d86ef3f8b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import platform\n","import datetime,pytz\n","\n","root_ = '/content/drive/My Drive/colab/' if platform.system() == 'Linux' else '/Users/love/Test/'\n","\n","WeiboSentiment_ = os.path.join(root_, 'WeiboSentiment')\n","if not os.path.exists(WeiboSentiment_):\n","    os.makedirs(WeiboSentiment_)\n","\n","model_ = os.path.join(WeiboSentiment_, 'model')\n","if not os.path.exists(model_):\n","    os.makedirs(model_)\n","\n","\n","import jieba\n","import re\n","import numpy as np\n","\n","def tokenize(text):\n","    \"\"\"\n","    带有语料清洗功能的分词函数, 包含数据预处理, 可以根据自己的需求重载\n","    \"\"\"\n","    text = re.sub(\"\\{%.+?%\\}\", \" \", text)           # 去除 {%xxx%} (地理定位, 微博话题等)\n","    text = re.sub(\"@.+?( |$)\", \" \", text)           # 去除 @xxx (用户名)\n","    text = re.sub(\"【.+?】\", \" \", text)              # 去除 【xx】 (里面的内容通常都不是用户自己写的)\n","    icons = re.findall(\"\\[.+?\\]\", text)             # 提取出所有表情图标\n","    text = re.sub(\"\\[.+?\\]\", \"IconMark\", text)      # 将文本中的图标替换为`IconMark`\n","\n","    tokens = []\n","    for k, w in enumerate(jieba.lcut(text)):\n","        w = w.strip()\n","        if \"IconMark\" in w:                         # 将IconMark替换为原图标\n","            for i in range(w.count(\"IconMark\")):\n","                tokens.append(icons.pop(0))\n","        elif w and w != '\\u200b' and w.isalpha():   # 只保留有效文本\n","                tokens.append(w)\n","    return tokens\n","\n","\n","def load_curpus(path):\n","    \"\"\"\n","    加载语料库\n","    \"\"\"\n","    data = []\n","    with open(path, \"r\", encoding=\"utf8\") as f:\n","        for line in f:\n","            [_, seniment, content] = line.split(\",\", 2)\n","            content = tokenize(content)             # 分词\n","            data.append((content, int(seniment)))\n","    return data\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"fAWE1aGyIX3U"},"source":["#### 加载数据"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"HiLhfWUwIX3V","executionInfo":{"status":"ok","timestamp":1601373470974,"user_tz":-420,"elapsed":46654,"user":{"displayName":"loveagri tang","photoUrl":"","userId":"02820716366022293214"}},"outputId":"d58c53a3-369e-45c1-fa6e-f3763fb2254d","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import pandas as pd\n","train_data = load_curpus(os.path.join(WeiboSentiment_, 'train.txt'))\n","test_data = load_curpus(os.path.join(WeiboSentiment_, 'test.txt'))\n","train_df = pd.DataFrame(train_data, columns=[\"content\", \"sentiment\"])\n","test_df = pd.DataFrame(test_data, columns=[\"content\", \"sentiment\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.983 seconds.\n","Prefix dict has been built successfully.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3ylw8TXWIX3Z"},"source":["stopwords = []\n","with open(os.path.join(WeiboSentiment_, 'stopwords.txt'), \"r\", encoding=\"utf8\") as f:\n","    for w in f:\n","        stopwords.append(w.strip())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flrJBOQrIX3c"},"source":["#### Ont-hot\n","ps: 与其他方法相比，朴素贝叶斯并没有对高质量词向量的需求，因此不再加载Fasttext词向量，而是直接用one-hot形式"]},{"cell_type":"code","metadata":{"id":"VpjX2UHvIX3d","executionInfo":{"status":"ok","timestamp":1601373474260,"user_tz":-420,"elapsed":49927,"user":{"displayName":"loveagri tang","photoUrl":"","userId":"02820716366022293214"}},"outputId":"cc2e59af-2409-4ece-8ed3-a48281e0f9a8","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","data_str = [\" \".join(content) for content, sentiment in train_data] + \\\n","            [\" \".join(content) for content, sentiment in test_data]\n","vectorizer = CountVectorizer(token_pattern='\\[?\\w+\\]?', stop_words=stopwords)\n","vectorizer.fit_transform(data_str)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['元', '吨', '数', '末'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<119988x117762 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 1138782 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"iPiU8BLFIX3g"},"source":["X_data, y_data = [], []\n","for content, sentiment in train_data:\n","    X, y = [], sentiment\n","    X_data.append(\" \".join(content))\n","    y_data.append(sentiment)\n","X_train = vectorizer.transform(X_data)\n","y_train = y_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VhlEOMmIX3j"},"source":["X_data, y_data = [], []\n","for content, sentiment in test_data:\n","    X, y = [], sentiment\n","    X_data.append(\" \".join(content))\n","    y_data.append(sentiment)\n","X_test = vectorizer.transform(X_data)\n","y_test = y_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"idv_PQMeIX3m"},"source":["#### Bayes\n","全部用默认参数"]},{"cell_type":"code","metadata":{"id":"iP-uUX6FIX3n","executionInfo":{"status":"ok","timestamp":1601373476329,"user_tz":-420,"elapsed":51982,"user":{"displayName":"loveagri tang","photoUrl":"","userId":"02820716366022293214"}},"outputId":"79718d9b-ae85-4aec-cf7b-8e3d5d5fa00a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"kY2l_bE4IX3q"},"source":["result = clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WUMeIEvvIX3t"},"source":["#### 模型评估\n","速度快，效果还很好，可能是因为该任务语料规模较小，在大规模语料任务上性能会下降"]},{"cell_type":"code","metadata":{"id":"EzqJr7y5IX3u","executionInfo":{"status":"ok","timestamp":1601373476331,"user_tz":-420,"elapsed":51973,"user":{"displayName":"loveagri tang","photoUrl":"","userId":"02820716366022293214"}},"outputId":"a6f698a5-cbed-49cd-8cdf-610768fb8bbe","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["from sklearn import metrics\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","print(metrics.classification_report(y_test, result))\n","\n","print('acc:',accuracy_score(y_test, result))\n","print('pc',precision_score(y_test, result))\n","print('rc:',recall_score(y_test, result))\n","print('f1:',f1_score(y_test, result))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.89      0.87      0.88     11884\n","           1       0.87      0.89      0.88     12113\n","\n","    accuracy                           0.88     23997\n","   macro avg       0.88      0.88      0.88     23997\n","weighted avg       0.88      0.88      0.88     23997\n","\n","acc: 0.8786514981039296\n","pc 0.8718176675018184\n","rc: 0.8905308346404689\n","f1: 0.8810748999428245\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"14BJqO_ZIX3w"},"source":[""],"execution_count":null,"outputs":[]}]}